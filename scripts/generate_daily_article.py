#!/usr/bin/env python3
"""
Daily Article Generation Script
Phase 10-1: Execute daily article generation workflow
"""

import asyncio
import os
import sys
from pathlib import Path

# Add parent directory to path
sys.path.insert(0, str(Path(__file__).parent.parent / "backend"))

from datetime import datetime

from database import SessionLocal
from models.automated_content import (AutomatedContentDB, ContentStatus,
                                      ContentType, TrendDataDB, ContentGenerationLogDB)
from services.content_automation_service import ContentAutomationService
from services.source_aggregator_service import SourceAggregatorService


async def main_async():
    """„É°„Ç§„É≥Âá¶ÁêÜÔºàÈùûÂêåÊúüÔºâ"""
    print("üöÄ Starting daily article generation...")

    db = SessionLocal()
    try:
        # Step 1: ÊÉÖÂ†±ÂèéÈõÜ
        print("üì° Collecting data from sources...")
        aggregator = SourceAggregatorService(db)
        source_data = await aggregator.collect_all_sources()
        print(f"‚úÖ Collected {len(source_data)} items")

        # Step 2: „Éà„É¨„É≥„ÉâÂàÜÊûê
        print("üìä Analyzing trends...")
        groq_api_key = os.getenv("GROQ_API_KEY", "")
        automation = ContentAutomationService(db, groq_api_key)
        trends = await automation.analyze_trends(source_data)
        print(f"‚úÖ Found {len(trends)} trends")

        # Step 3: „Éà„É¨„É≥„Éâ„Éá„Éº„Çø‰øùÂ≠ò
        print("üíæ Saving trend data...")
        for trend in trends[:5]:
            trend_db = TrendDataDB(
                trend_name=trend['keyword'],
                trend_score=float(trend['score']),
                source_count=trend['source_count'],
                keywords=[trend['keyword']],
                related_topics=[item.get('title', '') for item in trend.get('related_items', [])[:3]],
                data_snapshot=trend,
                detected_at=datetime.utcnow()
            )
            db.add(trend_db)
        db.commit()
        print(f"‚úÖ Saved {len(trends[:5])} trend data")

        # Step 4: Ë®ò‰∫ãÁîüÊàê„Å®‰øùÂ≠ò
        print("‚úçÔ∏è  Generating articles...")
        generated_count = 0
        skipped_count = 0
        
        for i, trend in enumerate(trends[:3], 1):  # „Éà„ÉÉ„Éó3Ë®ò‰∫ãÁîüÊàê
            print(f"  Generating article {i}/3: {trend['keyword']}")
            
            start_time = datetime.utcnow()
            article = await automation.generate_article(trend)
            
            # ÁîüÊàê„É≠„Ç∞‰øùÂ≠ò
            log = ContentGenerationLogDB(
                generation_type="daily_article",
                status="success" if article else "failed",
                api_cost=0.0,  # Groq„ÅØÁÑ°Êñô
                generation_time=article.get('generation_time', 0) if article else 0,
                quality_score=article.get('quality_score', 0) if article else 0
            )
            
            if article and article.get('quality_score', 0) >= 80:
                # „Çπ„É©„ÉÉ„Ç∞ÁîüÊàê
                slug = article['title'].lower().replace(' ', '-').replace('/', '-')[:100]
                
                # Ë®ò‰∫ã„ÇíDB„Å´‰øùÂ≠ò
                article_db = AutomatedContentDB(
                    content_type=ContentType.ARTICLE,
                    title=article['title'],
                    slug=slug,
                    summary=article.get('summary', '')[:500],
                    content=article['content'],
                    metadata={
                        'tags': article.get('tags', []),
                        'read_time': article.get('read_time', 5),
                        **article.get('metadata', {})
                    },
                    seo_data=article.get('seo_data', {}),
                    quality_score=article['quality_score'],
                    status=ContentStatus.PUBLISHED,
                    published_at=datetime.utcnow()
                )
                db.add(article_db)
                db.commit()
                
                log.content_id = article_db.id
                log.status = "success"
                generated_count += 1
                
                print(f"  ‚úÖ Generated & Saved: {article['title']}")
                print(f"     Score: {article['quality_score']:.1f} | ID: {article_db.id}")
            else:
                log.error_message = "Quality score < 80" if article else "Generation failed"
                log.status = "skipped" if article else "failed"
                skipped_count += 1
                print(f"  ‚ö†Ô∏è  Skipped (low quality or failed)")
            
            db.add(log)
            db.commit()

        print(f"\nüìä Results: {generated_count} generated, {skipped_count} skipped")

        print("\nüéâ Daily article generation completed!")

    except Exception as e:
        print(f"‚ùå Error: {e}")
        sys.exit(1)
    finally:
        db.close()


def main():
    """„Ç®„É≥„Éà„É™„Éº„Éù„Ç§„É≥„Éà"""
    asyncio.run(main_async())


if __name__ == "__main__":
    main()

